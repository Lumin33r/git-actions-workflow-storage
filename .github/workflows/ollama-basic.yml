name: Ollama AI Processing

on:
  push:
    branches: [master]
  workflow_dispatch:

jobs:
  ai-processing:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: System information
        run: |
          echo "Runner OS: ${{ runner.os }}"
          echo "Available disk space:"
          df -h
          echo "Available memory:"
          free -h
          echo "CPU information:"
          nproc

      - name: Cache Ollama installation
        uses: actions/cache@v3
        id: ollama-cache
        with:
          path: ~/ollama-bin
          key: ollama-install-${{ runner.os }}-v1

      - name: Install Ollama
        run: |
          if [ -f ~/ollama-bin/ollama ]; then
            echo "âœ“ Using cached Ollama installation"
            sudo cp ~/ollama-bin/ollama /usr/local/bin/ollama
            sudo chmod +x /usr/local/bin/ollama
          else
            echo "â†“ Installing Ollama (first run)..."
            curl -fsSL https://ollama.com/install.sh | sh
            mkdir -p ~/ollama-bin
            cp /usr/local/bin/ollama ~/ollama-bin/
            echo "âœ“ Cached Ollama binary for future runs"
          fi

      - name: Start Ollama service
        run: |
          ollama serve &
          sleep 10

      - name: Verify Ollama installation
        run: |
          ollama --version
          echo "Cache hit: ${{ steps.ollama-cache.outputs.cache-hit }}"
          if [ -f ~/ollama-bin/ollama ]; then
            echo "Binary size: $(du -h ~/ollama-bin/ollama | cut -f1)"
          fi

      - name: Run modular test suite
        id: modular-tests
        run: |
          echo "Running modular test suite..."
          python scripts/test_runner.py
        continue-on-error: true

      - name: Create timestamped result directory
        run: |
          python -c "
          import sys
          sys.path.append('.')
          from utils.directory_manager import DirectoryManager

          dm = DirectoryManager('results')
          result_dir = dm.create_timestamped_dir('run-${{ github.run_number }}')

          with open('current_result_dir.txt', 'w') as f:
              f.write(str(result_dir))

          print(f'Created result directory: {result_dir}')
          "

      - name: Download AI model
        run: |
          echo "Downloading llama3.2:1b model..."
          ollama pull llama3.2:1b

      - name: Verify model availability
        run: |
          echo "Available models:"
          ollama list
          echo "Model download complete"

      - name: Generate workflow results
        run: |
          RESULT_DIR=$(cat current_result_dir.txt)

          echo "Generating AI analysis..."
          WORKFLOW_FILE=".github/workflows/ollama-basic.yml"
          PROMPT="Analyze this GitHub Actions workflow and suggest three specific improvements for production use. Focus on security, performance, and maintainability."
          WORKFLOW_CONTENT=$(cat "$WORKFLOW_FILE")

          ollama run llama3.2:1b "$PROMPT

          $WORKFLOW_CONTENT" > "$RESULT_DIR/workflow-analysis.txt"

          echo "AI Analysis:"
          cat "$RESULT_DIR/workflow-analysis.txt"

          echo "Workflow Run: ${{ github.run_number }}" > "$RESULT_DIR/metadata.txt"
          echo "Timestamp: $(date)" >> "$RESULT_DIR/metadata.txt"
          echo "Commit: ${{ github.sha }}" >> "$RESULT_DIR/metadata.txt"

          echo "Results saved to $RESULT_DIR"
          ls -la "$RESULT_DIR"

      # ============================================
      # TESTING SECTION
      # ============================================

      - name: Setup Python testing environment
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run critical tests (fail fast)
        id: critical-tests
        run: |
          mkdir -p test-results
          python -m pytest tests/ -v \
            --junitxml=test-results/critical-results.xml \
            --cov=. \
            --cov-report=xml:test-results/coverage.xml \
            --cov-report=html:test-results/coverage-html \
            -x 2>&1 | tee test-results/critical-output.txt
          echo "CRITICAL_EXIT_CODE=$?" >> $GITHUB_ENV

      - name: Run reliability tests (advisory)
        id: advisory-tests
        continue-on-error: true
        run: |
          python -m pytest tests/test_reliability.py -v \
            --junitxml=test-results/reliability-results.xml \
            --timeout=120 \
            2>&1 | tee test-results/advisory-output.txt || true

      - name: Performance benchmarking
        id: performance
        run: |
          echo "=== Workflow Performance Report ===" > test-results/performance-report.txt
          echo "Timestamp: $(date)" >> test-results/performance-report.txt
          echo "Workflow Run: ${{ github.run_number }}" >> test-results/performance-report.txt
          echo "Cache Hit: ${{ steps.ollama-cache.outputs.cache-hit }}" >> test-results/performance-report.txt
          echo "" >> test-results/performance-report.txt

          echo "=== AI Response Timing ===" >> test-results/performance-report.txt

          # Test 1: Simple query
          start_time=$(date +%s.%N)
          ollama run llama3.2:1b "Say hello" > /dev/null 2>&1
          end_time=$(date +%s.%N)
          simple_time=$(echo "$end_time - $start_time" | bc)
          echo "Simple Query Time: ${simple_time}s" >> test-results/performance-report.txt
          echo "SIMPLE_QUERY_TIME=${simple_time}" >> $GITHUB_ENV

          # Test 2: Complex query
          start_time=$(date +%s.%N)
          ollama run llama3.2:1b "What is continuous integration?" > ai_response.txt
          end_time=$(date +%s.%N)
          complex_time=$(echo "$end_time - $start_time" | bc)
          echo "Complex Query Time: ${complex_time}s" >> test-results/performance-report.txt
          echo "Response Length: $(wc -w < ai_response.txt) words" >> test-results/performance-report.txt
          echo "COMPLEX_QUERY_TIME=${complex_time}" >> $GITHUB_ENV

          cat test-results/performance-report.txt

      - name: Generate test metrics
        if: always()
        run: |
          # Count tests from XML results
          CRITICAL_TOTAL=$(grep -oP 'tests="\K[0-9]+' test-results/critical-results.xml 2>/dev/null | head -1 || echo "0")
          CRITICAL_FAILURES=$(grep -oP 'failures="\K[0-9]+' test-results/critical-results.xml 2>/dev/null | head -1 || echo "0")
          CRITICAL_ERRORS=$(grep -oP 'errors="\K[0-9]+' test-results/critical-results.xml 2>/dev/null | head -1 || echo "0")
          CRITICAL_SKIPPED=$(grep -oP 'skipped="\K[0-9]+' test-results/critical-results.xml 2>/dev/null | head -1 || echo "0")
          CRITICAL_PASSED=$((CRITICAL_TOTAL - CRITICAL_FAILURES - CRITICAL_ERRORS - CRITICAL_SKIPPED))

          ADVISORY_TOTAL=$(grep -oP 'tests="\K[0-9]+' test-results/reliability-results.xml 2>/dev/null | head -1 || echo "0")
          ADVISORY_FAILURES=$(grep -oP 'failures="\K[0-9]+' test-results/reliability-results.xml 2>/dev/null | head -1 || echo "0")
          ADVISORY_ERRORS=$(grep -oP 'errors="\K[0-9]+' test-results/reliability-results.xml 2>/dev/null | head -1 || echo "0")
          ADVISORY_SKIPPED=$(grep -oP 'skipped="\K[0-9]+' test-results/reliability-results.xml 2>/dev/null | head -1 || echo "0")
          ADVISORY_PASSED=$((ADVISORY_TOTAL - ADVISORY_FAILURES - ADVISORY_ERRORS - ADVISORY_SKIPPED))

          TOTAL_TESTS=$((CRITICAL_TOTAL + ADVISORY_TOTAL))
          TOTAL_PASSED=$((CRITICAL_PASSED + ADVISORY_PASSED))
          TOTAL_FAILED=$((CRITICAL_FAILURES + CRITICAL_ERRORS + ADVISORY_FAILURES + ADVISORY_ERRORS))
          TOTAL_SKIPPED=$((CRITICAL_SKIPPED + ADVISORY_SKIPPED))

          # Save to environment for summary
          echo "TOTAL_TESTS=${TOTAL_TESTS}" >> $GITHUB_ENV
          echo "TOTAL_PASSED=${TOTAL_PASSED}" >> $GITHUB_ENV
          echo "TOTAL_FAILED=${TOTAL_FAILED}" >> $GITHUB_ENV
          echo "TOTAL_SKIPPED=${TOTAL_SKIPPED}" >> $GITHUB_ENV
          echo "CRITICAL_PASSED=${CRITICAL_PASSED}" >> $GITHUB_ENV
          echo "CRITICAL_FAILED=$((CRITICAL_FAILURES + CRITICAL_ERRORS))" >> $GITHUB_ENV
          echo "ADVISORY_PASSED=${ADVISORY_PASSED}" >> $GITHUB_ENV
          echo "ADVISORY_FAILED=$((ADVISORY_FAILURES + ADVISORY_ERRORS))" >> $GITHUB_ENV

          # Create metrics file
          cat > test-results/test_metrics.json << EOF
          {
            "workflow_run": "${{ github.run_number }}",
            "timestamp": "$(date -Iseconds)",
            "critical_tests": {
              "total": ${CRITICAL_TOTAL},
              "passed": ${CRITICAL_PASSED},
              "failed": $((CRITICAL_FAILURES + CRITICAL_ERRORS)),
              "skipped": ${CRITICAL_SKIPPED}
            },
            "advisory_tests": {
              "total": ${ADVISORY_TOTAL},
              "passed": ${ADVISORY_PASSED},
              "failed": $((ADVISORY_FAILURES + ADVISORY_ERRORS)),
              "skipped": ${ADVISORY_SKIPPED}
            },
            "totals": {
              "total": ${TOTAL_TESTS},
              "passed": ${TOTAL_PASSED},
              "failed": ${TOTAL_FAILED},
              "skipped": ${TOTAL_SKIPPED}
            },
            "performance": {
              "simple_query_time": "${SIMPLE_QUERY_TIME:-N/A}",
              "complex_query_time": "${COMPLEX_QUERY_TIME:-N/A}"
            }
          }
          EOF

          cat test-results/test_metrics.json

      - name: Write workflow summary
        if: always()
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          # ðŸ§ª Test Results Summary

          ## Run Information
          - **Workflow Run:** #${{ github.run_number }}
          - **Commit:** ${{ github.sha }}
          - **Branch:** ${{ github.ref_name }}
          - **Timestamp:** $(date)

          ## Test Results

          | Category | Total | âœ… Passed | âŒ Failed | â­ï¸ Skipped |
          |----------|-------|-----------|-----------|------------|
          EOF

          echo "| **Critical Tests** | ${CRITICAL_TOTAL:-0} | ${CRITICAL_PASSED:-0} | ${CRITICAL_FAILED:-0} | ${CRITICAL_SKIPPED:-0} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Advisory Tests** | ${ADVISORY_TOTAL:-0} | ${ADVISORY_PASSED:-0} | ${ADVISORY_FAILED:-0} | ${ADVISORY_SKIPPED:-0} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Total** | ${TOTAL_TESTS:-0} | ${TOTAL_PASSED:-0} | ${TOTAL_FAILED:-0} | ${TOTAL_SKIPPED:-0} |" >> $GITHUB_STEP_SUMMARY

          cat >> $GITHUB_STEP_SUMMARY << 'EOF'

          ## âš¡ Performance Metrics

          | Metric | Value |
          |--------|-------|
          EOF

          echo "| Simple Query Time | ${SIMPLE_QUERY_TIME:-N/A}s |" >> $GITHUB_STEP_SUMMARY
          echo "| Complex Query Time | ${COMPLEX_QUERY_TIME:-N/A}s |" >> $GITHUB_STEP_SUMMARY
          echo "| Cache Hit | ${{ steps.ollama-cache.outputs.cache-hit }} |" >> $GITHUB_STEP_SUMMARY

          cat >> $GITHUB_STEP_SUMMARY << 'EOF'

          ## ðŸ“¦ Artifacts

          The following artifacts are available for download:

          - **test-results-run-${{ github.run_number }}** - JUnit XML results, coverage reports, and metrics
          - **ai-responses-${{ github.run_number }}** - AI analysis outputs

          ### Coverage Report

          View the HTML coverage report in the artifacts for detailed code coverage analysis.

          ---

          EOF

          if [ "${TOTAL_FAILED:-0}" -gt 0 ]; then
            echo "âš ï¸ **Some tests failed.** Review the test results artifacts for details." >> $GITHUB_STEP_SUMMARY
          else
            echo "âœ… **All tests passed successfully!**" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload test results artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-run-${{ github.run_number }}
          path: |
            results/
            coverage.xml
          retention-days: 30

      - name: Upload AI responses
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ai-analysis-run-${{ github.run_number }}
          path: outputs/
          retention-days: 30

      - name: Upload workflow results
        uses: actions/upload-artifact@v4
        with:
          name: workflow-results-${{ github.run_number }}
          path: results/
          retention-days: 30

      # ============================================
      # GIT COMMIT SECTION
      # ============================================

      - name: Configure git for automated commits
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"

      - name: Commit results to repository
        run: |
          RESULT_DIR=$(cat current_result_dir.txt)

          git add results/
          git commit -m "Add results from workflow run #${{ github.run_number }}" || echo "No changes to commit"
          git push origin main || echo "Push failed - may need permissions"
        continue-on-error: true
